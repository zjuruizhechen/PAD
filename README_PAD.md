# PAD: Personalized Alignment at Decoding-time


This repository contains the official implementation of the paper PAD: Personalized Alignment at Decoding-time.
This paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. 

This repository is built based on https://github.com/deeplearning-wisc/args.


## Setup
The following packages, and versions were used:

```bash=
git clone https://github.com/deeplearning-wisc/args.git

conda create -n PAD python=3.10 -y
conda activate PAD

cd PAD
pip -r requirements.txt
```
We recommend the following versions for the main packages:

| Mandatory    | Recommend |
| ------------ |-----------|
| python       | 3.10      |
| torch        | 2.1.0     |
| transformers | 4.45.0    |
| datasets     | 2.16.0    |
| accelerate   | 0.34.2    |
| peft         | 0.14.0    |
| trl          | 0.9.6     |

## Inference
To run the inference script, first a configuration file must be made:

`echo '{rm_weight": 1.0, "topk": 10, "mode": "greedy", "sample_temp": null}' > example.config`
This example configuration runs `args-greedy` with `k=10` and `w=1.0`.


Then the following command can be run to start generation:

```bash
python collect_model_outs.py \
    --run_percent 100. \
    --config="configs/hh_llama3_PAD.config" \
    --PAD="PAD4" \
    --out_file="new_results/psoups_llama3SFT_DPO12_hhh4" \
    --llm_gpu="cuda:4" \
    --rm_gpu="cuda:5" \
    --llm="princeton-nlp/Llama-3-Base-8B-SFT" \
    --rm="../ckpt/PAD/saves/llama3-8b-Instruct-PAD_HelpSteerSafeRLHF/dpo12/reward_merged" \
    --dataset="psoups" \
    --sys_prompt="[Guidelines] Your task is to generate response by considering the following principle. \
[Principles] harmless and helpfulness and humor \
[Instruction] "
```

The final result of the generation will be stored as a jsonl file with the path `run_outs/example_out_0.jsonl` where the number at the end corresponds to the line number in the configuration file.

## Evaluations

To prepare for the evaluations, please extract your models output in the following form and save it as `outputs/your_run_name.json`. Note that the response should contain both prompt and the output generated by the model.

```jsonld
[
    {
        "prompt": "Are you okay? You look",
        "response": "Are you okay? You look a bit tired or stressed. Anything you'd like to talk about?",
        "method": "greedy"
    },
    {
        "prompt": "...",
        "response": "...",
        "method": "..."
    },
    ...
]
```

To run the evaluations, execute the following commands:

```bash


# helpfulness

python measure_reward.py \
    --out_file="<your_results>.jsonl" \
    --tokenizer="Ray2333/gpt2-large-helpful-reward_model" \
    --rm="Ray2333/gpt2-large-helpful-reward_model" \
    --rm_gpu="cuda:0"

python measure_reward.py \
    --out_file="<your_results>.jsonl" \
    --tokenizer="RLHFlow/ArmoRM-Llama3-8B-v0.1" \
    --rm="RLHFlow/ArmoRM-Llama3-8B-v0.1" \
    --rm_gpu="cuda:0" \
    --dimension=0

# harmless

python measure_reward.py \
    --out_file="<your_results>.jsonl" \
    --tokenizer="Ray2333/gpt2-large-harmless-reward_model" \
    --rm="Ray2333/gpt2-large-harmless-reward_model" \
    --rm_gpu="cuda:0"

python measure_reward.py \
    --out_file="<your_results>.jsonl" \
    --tokenizer="RLHFlow/ArmoRM-Llama3-8B-v0.1" \
    --rm="RLHFlow/ArmoRM-Llama3-8B-v0.1" \
    --rm_gpu="cuda:0" \
    --dimension=10

# humor

python measure_reward.py \
  --out_file="<your_results>.jsonl" \
  --tokenizer="mohameddhiab/humor-no-humor" \
  --rm="mohameddhiab/humor-no-humor" \
  --rm_gpu="cuda:0"
```

## Results
We conducted a performance comparison between ARGS and conventional decoding methods on Supervised Fine-Tuning (SFT) versions of Llama-7b and several OPT models. Remarkably, ARGS consistently outperformed the alternatives, including scenarios where the OPT model was trained using PPO.

![](https://hackmd.io/_uploads/r1Q2ZFlzp.png)
ARGS-Greedy Llama-7b performance

![](https://hackmd.io/_uploads/HyVn-Yefa.png)
ARGS-Greedy OPT Model performance



## Citation

If you find this repository useful in your research, please consider citing:

```
@inproceedings{khanov2024args,
    title={ARGS: Alignment as Reward-Guided Search},
    author={Maxim Khanov and Jirayu Burapacheep and Yixuan Li},
    booktitle={Proceedings of the International Conference on Learning Representations},
    year={2024}
}
```
